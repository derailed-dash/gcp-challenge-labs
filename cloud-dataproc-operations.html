<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Cloud Dataproc Cluster Operations and Maintenance | Dazbo’s GCP Skillsboost Challenge Lab Walkthroughs</title>
<meta name="generator" content="Jekyll v3.9.2" />
<meta property="og:title" content="Cloud Dataproc Cluster Operations and Maintenance" />
<meta name="author" content="Dazbo" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="My experience and walkthroughs with the GCP Skillsboost Challange Labs." />
<meta property="og:description" content="My experience and walkthroughs with the GCP Skillsboost Challange Labs." />
<link rel="canonical" href="https://derailed-dash.github.io/gcp-challenge-labs/cloud-dataproc-operations.html" />
<meta property="og:url" content="https://derailed-dash.github.io/gcp-challenge-labs/cloud-dataproc-operations.html" />
<meta property="og:site_name" content="Dazbo’s GCP Skillsboost Challenge Lab Walkthroughs" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Cloud Dataproc Cluster Operations and Maintenance" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Dazbo"},"description":"My experience and walkthroughs with the GCP Skillsboost Challange Labs.","headline":"Cloud Dataproc Cluster Operations and Maintenance","url":"https://derailed-dash.github.io/gcp-challenge-labs/cloud-dataproc-operations.html"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/gcp-challenge-labs/assets/css/style.css?v=e0cb695c2ee83a16a2375b87c164a13c944c1240">
    <script src="/gcp-challenge-labs/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <!-- Setup Google Analytics -->



<link rel="shortcut icon" type="image/x-icon" href="https://derailed-dash.github.io/gcp-challenge-labs/favicon.ico" >

<!-- end custom head snippets -->

  </head>
  <body>
    <div class="wrapper">
      <header >
        <h1>Dazbo's GCP Skillsboost Challenge Lab Walkthroughs</h1>
        
          <p>My experience and walkthroughs with the GCP Skillsboost Challange Labs.</p>
        
        <nav>
    <ol class="breadcrumb">
        
        
        <li class="breadcrumb-item">
            <a href="/gcp-challenge-labs/">Home</a>
        </li>
        
    </ol>
</nav>
        <ul>
        
          <li><a href="https://github.com/derailed-dash/derailed-dash.github.io" target="_blank">View On <strong>GitHub</strong></a></li>
        </ul>
      </header>
      <section>
      <h1 id="cloud-dataproc-cluster-operations-and-maintenance">Cloud Dataproc Cluster Operations and Maintenance</h1>

<p>See <a href="https://partner.cloudskillsboost.google/course_sessions/2416911/labs/343602" target="_blank">Challenge Lab</a></p>

<h2 id="page-contents">Page Contents</h2>

<ul>
  <li><a href="#overview">Overview</a></li>
  <li><a href="#objectives">Objectives</a></li>
  <li><a href="#whats-going-on">What’s Going On?</a></li>
  <li><a href="#approach">Approach</a></li>
  <li><a href="#steps">Steps</a></li>
</ul>

<h2 id="overview">Overview</h2>

<p><em>The Data Scientist team plans to port an existing predictive machine learning application to a Cloud Dataproc cluster. The application is written in Python and runs on Spark. The application takes a long time to run, even with sample data.</em></p>

<p><em>So the Data Scientists have established a benchmark by running a program in their data center. They have attempted to run the benchmark on a Cloud Dataproc cluster, but it is taking longer than they would like. Your job is to run the benchmark program on Cloud Dataproc and make adjustments to the cluster configuration to meet their requirements.</em></p>

<p>The benchmark application is a PySpark application that calculates the value of Pi.  It’s input parameter is the number of iterations it performs.  At the start of the lab, when the benchmark program is submitted with the input value of 20, the job completes in under 75 seconds. When it is submitted with the required input value of 220, the job takes about 2 minutes to run, which does not meet the requirement.</p>

<p><strong>Our goal: for the benchmark program to complete in under 75 seconds, with an input value of 220.</strong></p>

<h2 id="objectives">Objectives</h2>

<ol>
  <li><a href="#setup">Setup</a></li>
  <li><a href="#create-a-bucket-for-the-cluster">Create a bucket for the cluster</a></li>
  <li><a href="#create-the-dataproc-cluster">Create a cluster</a></li>
  <li><a href="#run-the-pyspark-benchmark-job">Run the PySpark benchmark job</a></li>
  <li><a href="#upgrade-the-master-node-configuration-and-re-run-the-job">Upgrade the master node configuration and re-run the job</a></li>
  <li><a href="#increase-the-number-of-worker-nodes-and-re-run-the-job">Increase the number of worker nodes and re-run the job</a></li>
</ol>

<h2 id="whats-going-on">What’s Going On?</h2>

<p>In this lab, we will deploy a Dataproc cluster with the required specification. We then use this cluster to execute an existing PySpark job, which the lab provides.  We know that this job will not run fast enough to meet the requirements of the lab. So this lab then tests our ability to do some rudimentary activities to increase the performance of the Dataproc cluster.  Specifically:</p>

<ul>
  <li>We will increase the size of the Dataproc cluster master GCE instance.</li>
  <li>We will increase the number of worker nodes in the cluster.</li>
</ul>

<p>After each change, we re-run the original job and see how fast it runs.</p>

<p>Note: you have to name your clusters, and job your job IDs exactly as specified in the lab.  If you don’t, the lab can’t verify that you’ve completed these steps.</p>

<h2 id="approach">Approach</h2>

<p>There are a few ways you could do this lab.  Perhaps the simplest approach is to use the Google Cloud Console to provision the bucket, provision the cluster, run the job, and make all the changes to the cluster.  I wanted to use the <strong>Cloud CLI</strong> as much as possible, so many of the steps I describe here will be done using <code class="language-plaintext highlighter-rouge">gcloud</code> in the <strong>Cloud Shell</strong>.</p>

<h2 id="steps">Steps</h2>

<h3 id="setup">Setup</h3>

<p>There is no setup required for this lab.  Just start your Cloud Shell.</p>

<h3 id="create-a-bucket-for-the-cluster">Create a bucket for the cluster</h3>

<p>Here we will create a bucket that Dataproc will use for its input data, and for its temporary data.  We’re told we need to name it after our project ID.  Then we’re told to copy the PySpark application from an existing location to our bucket. This is easy enough…</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># get our project ID</span>
<span class="nb">export </span><span class="nv">PROJECT_ID</span><span class="o">=</span><span class="si">$(</span>gcloud info <span class="nt">--format</span><span class="o">=</span><span class="s1">'value(config.project)'</span><span class="si">)</span>

<span class="c"># create our bucket</span>
gsutil mb gs://<span class="nv">$PROJECT_ID</span>

<span class="c"># copy the PySpark code from source bucket to our bucket</span>
gsutil <span class="nb">cp </span>gs://cloud-training/preppde/benchmark.py gs://<span class="nv">$PROJECT_ID</span>
</code></pre></div></div>

<h3 id="create-the-dataproc-cluster">Create the Dataproc cluster</h3>

<p>Here I just used the Cloud Console.  You just need to open <code class="language-plaintext highlighter-rouge">Dataproc</code> in the <code class="language-plaintext highlighter-rouge">Cloud Console</code>, create a new cluster, and ensure you specify the required parameters.  I.e.</p>

<ul>
  <li>Region: us-east1</li>
  <li>Zone: us-east1-b</li>
  <li>Version: 2.0-debian10</li>
  <li>Manager node: n1-standard-2</li>
  <li>Worker nodes: 2 of n1-standard-2</li>
  <li>Customise cluster -&gt; Cloud Storage Staging bucket: select the bucket you created above.</li>
</ul>

<p>(Your specific requirements might be different.)</p>

<p>It takes a few minutes to provision the cluster.  When it’s done, you can always take a look in <strong>VM Instances</strong> and you’ll see the three instances that make up the cluster.</p>

<h3 id="run-the-pyspark-benchmark-job">Run the PySpark benchmark job</h3>

<p>We’re told to run the job with an input parameter of <code class="language-plaintext highlighter-rouge">20</code>.  Here’s how you can do it with <code class="language-plaintext highlighter-rouge">gcloud</code>:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Note how we can use --id to specify a job id</span>
gcloud dataproc <span class="nb">jobs </span>submit pyspark <span class="nt">--id</span> mjtelco-test-1 <span class="se">\</span>
  <span class="nt">--cluster</span> mjtelco <span class="se">\</span>
  gs://<span class="nv">$PROJECT_ID</span>/benchmark.py <span class="se">\</span>
  <span class="nt">--region</span> us-east1 <span class="se">\</span>
  <span class="nt">--max-failures-per-hour</span><span class="o">=</span>1 <span class="se">\</span>
  <span class="nt">--</span> 20
</code></pre></div></div>

<p>Note that you have to give the job a specific job ID, as specified in the lab.  If you don’t specify this explicitly, then Dataproc just generates a job ID for you.  But you need to use the specified job ID to pass the lab step.  Although it’s obvious now, it took me a little while to work out how to specify a job ID!!</p>

<p>Notes:</p>
<ul>
  <li>We’re submitting a job of type <code class="language-plaintext highlighter-rouge">pyspark</code>.</li>
  <li>We’re specifying an explicit ID.</li>
  <li>We point the job to the py file in our bucket.</li>
  <li>We pass the job a single parameter: <code class="language-plaintext highlighter-rouge">20</code>.</li>
</ul>

<p>The job runs pretty quickly.</p>

<p>We’re then told to run the job again, but this time with an input parameter of <code class="language-plaintext highlighter-rouge">220</code>.  So we run the same command, but with a new job ID and the new parameter value:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>gcloud dataproc <span class="nb">jobs </span>submit pyspark <span class="nt">--id</span> mjtelco-test-2 <span class="se">\</span>
  <span class="nt">--cluster</span> mjtelco <span class="se">\</span>
  gs://<span class="nv">$PROJECT_ID</span>/benchmark.py <span class="se">\</span>
  <span class="nt">--region</span> us-east1 <span class="se">\</span>
  <span class="nt">--max-failures-per-hour</span><span class="o">=</span>1 <span class="se">\</span>
  <span class="nt">--</span> 220
</code></pre></div></div>

<p>From the Dataproc <strong>Jobs</strong> page, you can see the job progress.  You’ll see it takes much longer to run.</p>

<p><img src="/gcp-challenge-labs/assets/images/dataproc-pi-job1-and-2.png" alt="Dataproc Pi Jobs 1 and 2" style="width:800px;" /></p>

<h3 id="upgrade-the-master-node-configuration-and-re-run-the-job">Upgrade the master node configuration and re-run the job</h3>

<p>There are a couple of ways you can change the spec of the master node instance.  You could destroy the cluster and recreate it.  But my approach was:</p>

<ol>
  <li><strong>Stop the master instance</strong>, from the <strong>VM Instances</strong> page in the Console.</li>
  <li><strong>Edit the instance</strong>, and change its <strong>machine type</strong> to the required type, e.g. <code class="language-plaintext highlighter-rouge">n1-standard-4</code>.</li>
  <li><strong>Start the instance</strong>.</li>
</ol>

<p>This can all be done in a couple of minutes.</p>

<p>Now let’s re-run the job.  Note that we’re changing the job ID again:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>gcloud dataproc <span class="nb">jobs </span>submit pyspark <span class="nt">--id</span> mjtelco-test-3 <span class="se">\</span>
  <span class="nt">--cluster</span> mjtelco <span class="se">\</span>
  gs://<span class="nv">$PROJECT_ID</span>/benchmark.py <span class="se">\</span>
  <span class="nt">--region</span> us-east1 <span class="se">\</span>
  <span class="nt">--max-failures-per-hour</span><span class="o">=</span>1 <span class="se">\</span>
  <span class="nt">--</span> 220
</code></pre></div></div>

<p>What you’ll probably find is that this has made very little difference to the overall job time.</p>

<h3 id="increase-the-number-of-worker-nodes-and-re-run-the-job">Increase the number of worker nodes and re-run the job</h3>

<p>Now we’ll increase the number of worker nodes from <code class="language-plaintext highlighter-rouge">2</code> to <code class="language-plaintext highlighter-rouge">5</code>, giving us more parallel processing capability in the cluster.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>gcloud dataproc clusters update mjtelco <span class="se">\</span>
  <span class="nt">--region</span> us-east1 <span class="se">\</span>
  <span class="nt">--num-workers</span> 5
</code></pre></div></div>

<p>It takes a couple of minutes to add and start the extra worker nodes. You can monitor progress in the <strong>VM instances</strong> page:</p>

<p><img src="/gcp-challenge-labs/assets/images/dataproc-add-instances.png" alt="Dataproc add instances" style="width:420px;" /></p>

<p>Now, we can run our job one last time:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>gcloud dataproc <span class="nb">jobs </span>submit pyspark <span class="nt">--id</span> mjtelco-test-4 <span class="se">\</span>
  <span class="nt">--cluster</span> mjtelco <span class="se">\</span>
  gs://<span class="nv">$PROJECT_ID</span>/benchmark.py <span class="se">\</span>
  <span class="nt">--region</span> us-east1 <span class="se">\</span>
  <span class="nt">--max-failures-per-hour</span><span class="o">=</span>1 <span class="se">\</span>
  <span class="nt">--</span> 220
</code></pre></div></div>

<p>And this time it runs in under 75 seconds.  Woop!</p>


      </section>
    </div>
    <footer>
    
      <p>Hosted on GitHub Pages</p>
    </footer>
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->
  </body>
</html>
